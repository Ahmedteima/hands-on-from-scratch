
---

##  Artificial Neural Network â€” From Scratch

This folder contains a simple implementation of an Artificial Neural Network (ANN) built **from scratch using NumPy** â€” without deep-learning frameworks.
The goal is to understand how forward propagation, backpropagation, and gradient-based learning really work under the hood.

---

###  Contents

* **ANN From Scratch.ipynb** â€” full notebook (training + explanations)
* **Forward Propagation For ANN.jpeg** â€” diagram explaining forward pass
* **BackPropagation_ANN.jpeg** â€” diagram explaining gradients
* **Sigmoid & derivative graphs** â€” visual intuition for the activation function

---

###  Model Architecture

* Input layer: features
* Hidden layer: sigmoid activation
* Output layer: sigmoid (binary classification)



---

###  Learning (Backpropagation)

Weights are updated using gradient descent:

[
W := W - \alpha \cdot \frac{\partial L}{\partial W}
]

where ( \alpha ) is the learning rate and ( L ) is binary cross-entropy loss.

---

###  What you will learn

forward propagation step-by-step
computing gradients manually
implementing backpropagation
training a network on small datasets
evaluating predictions with metrics

---

###  How to run

1. Open the notebook in Jupyter or VS Code
2. Run cells in order
3. Try changing:

* hidden layer size
* learning rate
* number of epochs

â€¦and observe how the model behaves.

---

###  Next steps (planned)

* add multi-class version
* add ReLU activation
* compare with logistic regression
* visualization of loss curve over epochs

---

Ù„Ùˆ ØªØ­Ø¨ØŒ Ø£Ø¹Ø¯Ù‘Ù„ Ø§Ù„Ù†Øµ Ø¨Ø­ÙŠØ« ÙŠÙƒÙˆÙ†:
ğŸ”¹ Ø¹Ø±Ø¨ÙŠ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ â€” Ø£Ùˆ
ğŸ”¹ Ø£Ù‚ØµØ±/Ø£Ø·ÙˆÙ„ â€” Ø£Ùˆ
ğŸ”¹ Ù…Ø®ØµØµ Ø¨Ø§Ù„Ø¸Ø¨Ø· Ù„Ø´Ø±Ø­ ÙƒÙˆØ¯Ùƒ Ø®Ø·ÙˆØ© Ø®Ø·ÙˆØ©.

ÙˆÙ‚ÙˆÙ„Ù‘ÙŠ ÙƒÙ…Ø§Ù† Ù„Ùˆ Ø¹Ø§ÙŠØ² README Ù„Ù„Ø±ÙŠØ¨Ùˆ ÙƒÙ„Ù‡ Ø¨Ø´ÙƒÙ„ Ø§Ø­ØªØ±Ø§ÙÙŠ ğŸ‘
